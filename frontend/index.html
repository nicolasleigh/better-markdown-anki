<html class="linux chrome" dir="ltr" data-bs-theme="light">
    <head>
    <meta charset="utf-8">
    <title>card layout</title>
<!-- <base href="http://127.0.0.1:45901/"><link rel="stylesheet" type="text/css" href="http://127.0.0.1:45901/_anki/css/webview.css"> -->
<script type="module" src="/src/main.jsx"></script>
</head>

<body class="card card1 isLin fancy">
    
<script src="http://127.0.0.1:45901/_anki/js/webview.js"></script>
<script src="http://127.0.0.1:45901/_anki/js/mathjax.js"></script>
<script src="http://127.0.0.1:45901/_anki/js/vendor/mathjax/tex-chtml-full.js"></script>
<script src="http://127.0.0.1:45901/_anki/js/reviewer.js"></script>

<div id="_mark" hidden="">★</div>
<div id="_flag" hidden="">⚑</div>
<div id="front-card-cloze">
\$

$$123\$$$

$123 \$ 567$

## Matrices: Representation of Vectors and Linear Maps

Matrices allow explicit manipulation of finite-dimensional vector spaces and linear maps.

`<span class="cloze" data-cloze="a1:am" data-ordinal="1">[inline]</span>`
`<span class="cloze" data-cloze="a1:am" data-ordinal="1">[inline2]</span>`


A vector $(a _ 1, \dots, a _ m)$ can be represented by a <span class="cloze" data-cloze="column matrix" data-ordinal="1">[...]</span>:
```
<span class="cloze" data-cloze="a1:am" data-ordinal="1">[...]</span>
```

If $f$ is a linear map from $W$ (with basis $w _ 1, \dots, w _ n$) to $V$ (with basis $v _ 1, \dots, v _ m$), and $f(w _ j) = a _ {1,j}v _ 1 + \dots + a _ {m,j}v _ m$, then $f$ is represented by the matrix:
```
<span class="cloze" data-cloze="a1,1   ...   a1,n
  :            :
am,1   ...   am,n" data-ordinal="1">[...]</span>
```
with $m$ rows and $n$ columns.

```
a1,1   ...   a1,n
  :            :
am,1   ...   am,n
```

</div>


<!-- <div id="front-card-cloze">
| 123 | ABC |
| -- | -- |
| Important | Cool |
| Yes | NO| 

# A Dummy Card Subtitle<br><br>```php<br><br>$x = "teste";<br>$a = 123<br>```
<br>&nbsp;```php
<br>$x =  "teste";
<br>```<br><br>### Math<br>- <span class="cloze" data-cloze="$\pi=3.1415...$" data-ordinal="1">[$\pi=...$]</span><br>### Escaping dollar<br><br>$$1\$ &lt; 1€$$ <br>after<br><br>### Escaped c++ code :<br><br>```cpp<br>#include &lt;stdio.h&gt;<br><span class="cloze-inactive" data-ordinal="2">boost:\:add_vertex</span>(...);<br>```<br><br>### Python<br><br>```py<br>def print_hello():<br>&nbsp; print("<span class="cloze-inactive" data-ordinal="2">hello</span>")<br>```

```python<div><div>def train(train_loader, model, disc_optim, gen_optim, disc_loss_func, gen_loss_func, num_epochs, device, show_every=100):<br>&nbsp;&nbsp;&nbsp; """<br>&nbsp;&nbsp;&nbsp; Training Procedure for GANs.<br>&nbsp;&nbsp;&nbsp; Input:<br>&nbsp;&nbsp;&nbsp; - train_loader: training loader.<br>&nbsp;&nbsp;&nbsp; - model: GAN model to be training.<br>&nbsp;&nbsp;&nbsp; - disc_optim: optimizer for the Discriminator, e.g. Adam.<br>&nbsp;&nbsp;&nbsp; - gen_optim: optimizer for the Generator, e.g. Adam.<br>&nbsp;&nbsp;&nbsp; - disc_loss_func: loss function for the discriminator, e.g. BCE Loss.<br>&nbsp;&nbsp;&nbsp; - gen_loss_func: loss function for the generator, e.g. BCE Loss.<br>&nbsp;&nbsp;&nbsp; - num_epochs: number of epochs to train the model.<br>&nbsp;&nbsp;&nbsp; - device: gpu device to be used for training the model.<br>&nbsp;&nbsp;&nbsp; - show_variable:<br><br>&nbsp;&nbsp;&nbsp; Output:<br>&nbsp;&nbsp;&nbsp; - disc_cost: accumulated costs from the discriminator.<br>&nbsp;&nbsp;&nbsp; - gen_costs: accumulated costs from the generator.<br>&nbsp;&nbsp;&nbsp; - fake_imgs: generated fake images from the generator.<br>&nbsp;&nbsp;&nbsp; """<br>&nbsp;&nbsp;&nbsp; start_time = time.time()<br><br>&nbsp;&nbsp;&nbsp; disc_costs = []<br>&nbsp;&nbsp;&nbsp; gen_costs = []<br>&nbsp;&nbsp;&nbsp; fake_imgs_all = []<br>&nbsp;&nbsp;&nbsp; fake_imgs_fixed = []<br><br>&nbsp;&nbsp;&nbsp; loss_function = nn.BCELoss()<br><br>&nbsp;&nbsp;&nbsp; fixed_latent_z = torch.randn(64, latent_dim, 1, 1, device=device)<br><br>&nbsp;&nbsp;&nbsp; for epoch in range(num_epochs):<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; model = model.train()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for batch_idx, (real_data, targets) in enumerate(train_loader):<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size = targets.size(0)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; real_data = real_data.to(device)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; targets = targets.to(device)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # CREATING GROUND-TRUTH<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # create real (1's) and fake (0's) labels to be used in the loss functions<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_real = <span class="cloze-inactive" data-ordinal="1">torch.ones(batch_size).float().to(device)</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_fake = <span class="cloze-inactive" data-ordinal="1">torch.zeros(batch_size).float().to(device)</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # TRAIN DISCRIMINATOR<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # reset the gradients<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_optim.zero_grad()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # train the discriminator to correctly classify real images<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_pred_real = model.disc_forward(<span class="cloze-inactive" data-ordinal="1">real_data</span>).view(-1)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; real_loss = <span class="cloze-inactive" data-ordinal="1">disc_loss_func(disc_pred_real, y_real)</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # generate a batch of images from samples of the latent prior<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="cloze-inactive" data-ordinal="1">z = torch.randn(batch_size, latent_dim, 1, 1, device=device )</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fake_data = <span class="cloze-inactive" data-ordinal="1">model.gen_forward(z)</span><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # train the discriminator to correctly classify fake images<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # (note: we need. to detach the computation graph of the generator<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # so that gradients are not backpropagated into the generator)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_pred_fake = model.disc_forward(fake_data.detach()).view(-1)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fake_loss = <span class="cloze-inactive" data-ordinal="1">disc_loss_func(disc_pred_fake, y_fake)</span><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # averaging real and fake loss<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_loss = 0.5*(real_loss + fake_loss)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # backward propagation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_loss.backward()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update discriminator weights<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_optim.step()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # TRAIN GENERATOR<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # reset the gradients<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_optim.zero_grad()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # train the generator to output an image that is classified as real<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_pred_fake = model.<span class="cloze-inactive" data-ordinal="2">disc_forward</span>(fake_data).view(-1)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # compute loss<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_loss = gen_loss_func(<span class="cloze" data-cloze="disc_pred_fake, y_real" data-ordinal="3">[...]</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # backward propagation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_loss.backward()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update generator weights<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_optim.step()<br><br>&nbsp;&nbsp;&nbsp; return disc_costs, gen_costs, fake_imgs_all, fake_imgs_fixed<br></div><div>```</div></div>

</div>

<div id="back-card-cloze">
    
```py<br>a = 3<br>```<br><br>```python<br>print("hello")<br>a = 3<br>```<br><br>```py<br>def train(train_loader, model, disc_optim, gen_optim, disc_loss_func, gen_loss_func, num_epochs, device, show_every=100):<div><div>&nbsp;&nbsp;&nbsp; """<br>&nbsp;&nbsp;&nbsp; Training Procedure for GANs.<br>&nbsp;&nbsp;&nbsp; Input:<br>&nbsp;&nbsp;&nbsp; - train_loader: training loader.<br>&nbsp;&nbsp;&nbsp; - model: GAN model to be training.<br>&nbsp;&nbsp;&nbsp; - disc_optim: optimizer for the Discriminator, e.g. Adam.<br>&nbsp;&nbsp;&nbsp; - gen_optim: optimizer for the Generator, e.g. Adam.<br>&nbsp;&nbsp;&nbsp; - disc_loss_func: loss function for the discriminator, e.g. BCE Loss.<br>&nbsp;&nbsp;&nbsp; - gen_loss_func: loss function for the generator, e.g. BCE Loss.<br>&nbsp;&nbsp;&nbsp; - num_epochs: number of epochs to train the model.<br>&nbsp;&nbsp;&nbsp; - device: gpu device to be used for training the model.<br>&nbsp;&nbsp;&nbsp; - show_variable:<br><br>&nbsp;&nbsp;&nbsp; Output:<br>&nbsp;&nbsp;&nbsp; - disc_cost: accumulated costs from the discriminator.<br>&nbsp;&nbsp;&nbsp; - gen_costs: accumulated costs from the generator.<br>&nbsp;&nbsp;&nbsp; - fake_imgs: generated fake images from the generator.<br>&nbsp;&nbsp;&nbsp; """<br>&nbsp;&nbsp;&nbsp; start_time = time.time()<br><br>&nbsp;&nbsp;&nbsp; disc_costs = []<br>&nbsp;&nbsp;&nbsp; gen_costs = []<br>&nbsp;&nbsp;&nbsp; fake_imgs_all = []<br>&nbsp;&nbsp;&nbsp; fake_imgs_fixed = []<br><br>&nbsp;&nbsp;&nbsp; loss_function = nn.BCELoss()<br><br>&nbsp;&nbsp;&nbsp; fixed_latent_z = torch.randn(64, latent_dim, 1, 1, device=device)<br><br>&nbsp;&nbsp;&nbsp; for epoch in range(num_epochs):<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; model = model.train()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for batch_idx, (real_data, targets) in enumerate(train_loader):<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size = targets.size(0)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; real_data = real_data.to(device)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; targets = targets.to(device)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # CREATING GROUND-TRUTH<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # create real (1's) and fake (0's) labels to be used in the loss functions<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_real = <span class="cloze" data-ordinal="1">torch.ones(batch_size).float().to(device)</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_fake = <span class="cloze" data-ordinal="1">torch.zeros(batch_size).float().to(device)</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # TRAIN DISCRIMINATOR<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # reset the gradients<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_optim.zero_grad()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # train the discriminator to correctly classify real images<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_pred_real = model.disc_forward(<span class="cloze" data-ordinal="1">real_data</span>).view(-1)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; real_loss = <span class="cloze" data-ordinal="1">disc_loss_func(disc_pred_real, y_real)</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # generate a batch of images from samples of the latent prior<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="cloze" data-ordinal="1">z = torch.randn(batch_size, latent_dim, 1, 1, device=device )</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fake_data = <span class="cloze" data-ordinal="1">model.gen_forward(z)</span><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # train the discriminator to correctly classify fake images<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # (note: we need. to detach the computation graph of the generator<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # so that gradients are not backpropagated into the generator)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_pred_fake = model.disc_forward(fake_data.detach()).view(-1)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fake_loss = <span class="cloze" data-ordinal="1">disc_loss_func(disc_pred_fake, y_fake)</span><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # averaging real and fake loss<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_loss = 0.5*(real_loss + fake_loss)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # backward propagation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_loss.backward()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update discriminator weights<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_optim.step()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # TRAIN GENERATOR<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ########################################################################<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # reset the gradients<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_optim.zero_grad()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # train the generator to output an image that is classified as real<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; disc_pred_fake = model.<span class="cloze-inactive" data-ordinal="2">disc_forward</span>(fake_data).view(-1)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # compute loss<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_loss = gen_loss_func(<span class="cloze-inactive" data-ordinal="3">disc_pred_fake, y_real</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # backward propagation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_loss.backward()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update generator weights<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gen_optim.step()<br><br>&nbsp;&nbsp;&nbsp; return disc_costs, gen_costs, fake_imgs_all, fake_imgs_fixed<br></div></div><div>```</div><div><br></div><div>123</div>
</div>
<div id="extra-card-cloze" class="field">
<span class="cloze" data-cloze="$\pi=3.1415...$" data-ordinal="1">[$\pi=...$]</span><br>### Escaping dollar<br><br>$$1\$ &lt; 1€$$ <br>after<br><br>### Escaped c++ code :<br><br>```cpp<br>#include &lt;stdio.h&gt;<br><span class="cloze-inactive" data-ordinal="2">boost:\:add_vertex</span>(...);<br>```<br><br>### Python<br><br>```py<br>def print_hello():<br>&nbsp; print("<span class="cloze-inactive" data-ordinal="2">hello</span>")<br>```
    This indicates that even with optimized attention mechanisms, there are still memory limitations that can be hit at very long sequence lengths, leading to a sharp decline in performance.
</div>

<div id="tags-card" class="field">
    Computer_Science::Benchmarking Computer_Science::GPU_Computing Deep_Learning::Performance_Metrics
</div>

<div id="difficulty-card" class="field">
    2/10
</div> -->


<!-- <div id="front-card-basic">
# Hello<br><br>## How are you?
</div>

<div id="back-card-basic">
Both PCA and k-means clustering are solving the same fundamental problem of **compressing data with maximum fidelity** (minimizing reconstruction error), but under **different constraints** on the model complexity. They can be unified under a broader class of techniques known as **matrix factorization methods**.
</div>
<div id="extra-card-basic">
$123\$ 123$

In PCA, the matrix $W$ (principal components) must be orthogonal, while in k-means, $W$ (cluster centroids) is arbitrary, and $Z$ (cluster assignments) encodes which cluster each point belongs to.
</div> -->


</div> 
</body></html>


